{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPGL3wsw9ceV",
        "outputId": "e04e2609-f508-49b5-ec4b-879682ba0993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ArmXq09nc6",
        "outputId": "bff5e5c4-07f1-49a2-df9e-58e9881e1ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAW4MBPY9t_P",
        "outputId": "7497959f-8c9b-44c9-8bd1-6cc9a1bcfda3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.33.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.44.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn lightgbm streamlit matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YGDfLCpZTGA",
        "outputId": "5761fd4f-64b0-4930-c9ad-913463f3099b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.44.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.1)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (4.13.1)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.1)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (1.71.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.45.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
            "Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, pyngrok, protobuf, numpy, keras, ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.1 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 numpy-1.26.4 protobuf-4.25.6 pyngrok-7.2.3 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.15.1 pandas numpy matplotlib seaborn scipy scikit-learn lightgbm streamlit pyngrok\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAVoXw1qep3D"
      },
      "source": [
        "# Plastic Injection Moulding Quality Prediction\n",
        "\n",
        "## Overview\n",
        "This project aims to predict the quality of plastic injection moulded parts using machine learning models. The dataset (`dataset.csv`) contains features related to the moulding process (e.g., `Melt temperature`, `Mold temperature`, `time_to_fill`) and a target variable `quality` (1=Waste, 2=Acceptable, 3=Target, 4=Inefficient). The project is divided into four sections:\n",
        "\n",
        "- **Section 1 (15%)**: Data Preprocessing & Exploratory Data Analysis (EDA)\n",
        "- **Section 2 (15%)**: Hypothesis Testing & ANOVA\n",
        "- **Section 3 (40%)**: Machine Learning Model Development (including ANN, which is mandatory)\n",
        "- **Section 4 (20%)**: Interactive Dashboard Development using Streamlit\n",
        "\n",
        "## Environment Setup\n",
        "The script uses TensorFlow for the ANN model, which has caused issues locally on Windows (Python 3.9) due to version compatibility and DLL errors. TensorFlow 2.10.0 is not available for Python 3.9, so we’ll use TensorFlow 2.12.0, the earliest compatible version. If DLL errors persist, run this script in WSL or Google Colab, where TensorFlow worked previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL2oMNZFfSoV"
      },
      "source": [
        "## Section 0: Import Libraries\n",
        "### Purpose\n",
        "Import all necessary libraries for data handling, visualization, statistical analysis, and machine learning.\n",
        "\n",
        "### Thought Process\n",
        "- `pandas` and `numpy` for data manipulation.\n",
        "- `matplotlib` and `seaborn` for visualization.\n",
        "- `scipy.stats` for ANOVA.\n",
        "- `sklearn` for preprocessing, model training, and evaluation.\n",
        "- `lightgbm` for the LightGBM model.\n",
        "- `tensorflow` for the ANN model (mandatory).\n",
        "- `warnings` to suppress unnecessary warnings.\n",
        "- Set a random seed with `np.random.seed(42)` for reproducibility.\n",
        "\n",
        "### Expected Observation\n",
        "Libraries should import successfully. TensorFlow 2.12.0 is used to ensure compatibility with Python 3.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpdjrs3NfUyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a1f14633-5fe4-4ce7-8a44-c3b91920b9c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-354c5e82d755>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Section 0: Import Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# Section 0: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import f_oneway, skew, kurtosis\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from lightgbm import LGBMClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JmuPw9c1FiT"
      },
      "source": [
        "## Section 0: Load Dataset\n",
        "### Purpose\n",
        "Load the `Dataset.csv` file, which contains the features and target variable `quality`.\n",
        "\n",
        "### Thought Process\n",
        "- Use `latin1` encoding to handle potential encoding issues with the CSV file.\n",
        "- The dataset should have columns like `Melt temperature`, `Mold temperature`, etc., and `quality` as the target.\n",
        "\n",
        "### Expected Observation\n",
        "Dataset should load successfully. Ensure the file is in the same directory as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wALMOW0s1G9d"
      },
      "outputs": [],
      "source": [
        "# Section 0: Load Dataset\n",
        "data = pd.read_csv('Dataset.csv', encoding='latin1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6RULQ-y1Uj4"
      },
      "source": [
        "## Section 1: Data Preprocessing & EDA (15%)\n",
        "### Dataset Overview\n",
        "#### Purpose\n",
        "Understand the dataset's structure, check for missing values, and explore the data distribution.\n",
        "\n",
        "#### Thought Process\n",
        "- Check the shape to understand the number of rows and columns.\n",
        "- Check data types to ensure features are numerical.\n",
        "- Check for missing values to decide if imputation is needed.\n",
        "\n",
        "#### Expected Observation\n",
        "The dataset should have multiple features and a `quality` target. Expect no missing values based on prior runs, but confirm here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHJKGghp1X0j"
      },
      "outputs": [],
      "source": [
        "# Section 1: Data Preprocessing & EDA (15%)\n",
        "## Dataset Overview\n",
        "print(\"### Dataset Overview\")\n",
        "print(\"Shape:\", data.shape)\n",
        "print(\"\\nData Types:\\n\", data.dtypes)\n",
        "print(\"\\nMissing Values:\\n\", data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhLO622lL5iR"
      },
      "source": [
        "### Feature Engineering\n",
        "#### Purpose\n",
        "Create a new feature to potentially improve model performance.\n",
        "\n",
        "#### Thought Process\n",
        "- Create a new feature `pressure_ratio` by dividing `APVs - Specific injection pressure peak value` by `APSs - Specific back pressure peak value`.\n",
        "- This ratio may capture the relative pressure dynamics, potentially improving model performance.\n",
        "\n",
        "#### Expected Observation\n",
        "The new `pressure_ratio` feature should be added to the dataset, which may help models capture interactions between pressure-related features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBpIiVESL7uD"
      },
      "outputs": [],
      "source": [
        "## Feature Engineering\n",
        "print(\"\\n### Feature Engineering\")\n",
        "# Calculate pressure_ratio\n",
        "data['pressure_ratio'] = data['APVs - Specific injection pressure peak value'] / data['APSs - Specific back pressure peak value']\n",
        "print(\"New feature 'pressure_ratio' added to the dataset.\")\n",
        "print(\"Sample values of 'pressure_ratio':\\n\", data['pressure_ratio'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zFJAEzk1gyz"
      },
      "source": [
        "### Statistical Summary\n",
        "#### Purpose\n",
        "Compute basic statistics (mean, std, min, max) and check skewness and kurtosis to understand data distribution.\n",
        "\n",
        "#### Thought Process\n",
        "- Skewness indicates asymmetry (positive/negative skew).\n",
        "- Kurtosis indicates the 'tailedness' of the distribution (high kurtosis = heavy tails).\n",
        "\n",
        "#### Expected Observation\n",
        "Features like `Melt temperature` may show varying skewness and kurtosis, indicating potential non-normality. This informs the need for standardization later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzaMZ7B21j1T"
      },
      "outputs": [],
      "source": [
        "## Statistical Summary\n",
        "print(\"\\n### Statistical Summary\")\n",
        "stats = data.describe()\n",
        "print(stats)\n",
        "for col in data.columns[:-1]:  # Exclude 'quality'\n",
        "    print(f\"{col} - Skewness: {skew(data[col]):.2f}, Kurtosis: {kurtosis(data[col]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVAmz4FjhkK5"
      },
      "outputs": [],
      "source": [
        "## Data Preprocessing\n",
        "# Load the dataset with encoding handling\n",
        "try:\n",
        "    data = pd.read_csv('Dataset.csv', encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    data = pd.read_csv('Dataset.csv', encoding='latin1')\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop('quality', axis=1)\n",
        "y = data['quality']\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Features (X) shape:\", X.shape)\n",
        "print(\"Target (y) shape:\", y.shape)\n",
        "print(\"Scaled features (X_scaled) shape:\", X_scaled.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSPLXIha11Bu"
      },
      "source": [
        "### Visualizations\n",
        "#### Purpose\n",
        "Visualize key features to understand their distributions and relationships with the target.\n",
        "\n",
        "#### Thought Process\n",
        "- Histograms with KDE for `Melt temperature`, `Mold temperature`, `time_to_fill`, and `ZDx - Plasticizing time` to check distributions.\n",
        "- Boxplot of `Melt temperature` vs `quality` to see how this feature varies across quality levels.\n",
        "\n",
        "#### Expected Observation\n",
        "Histograms may reveal non-normal distributions (e.g., skewed `time_to_fill`). The boxplot should show if `Melt temperature` varies significantly across quality levels, suggesting its predictive power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjKUaLLJ1109"
      },
      "outputs": [],
      "source": [
        "## Additional EDA Visualizations\n",
        "print(\"\\n### Additional EDA Visualizations\")\n",
        "\n",
        "# Histogram of pressure_ratio\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(data['pressure_ratio'], kde=True, color='blue')\n",
        "plt.title('Distribution of Pressure Ratio')\n",
        "plt.xlabel('Pressure Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Boxplots of key features by quality\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle('Feature Distributions by Quality', fontsize=16)\n",
        "\n",
        "sns.boxplot(x='quality', y='Melt temperature', data=data, ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Melt Temperature by Quality')\n",
        "\n",
        "sns.boxplot(x='quality', y='Mold temperature', data=data, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Mold Temperature by Quality')\n",
        "\n",
        "sns.boxplot(x='quality', y='time_to_fill', data=data, ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Time to Fill by Quality')\n",
        "\n",
        "sns.boxplot(x='quality', y='pressure_ratio', data=data, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Pressure Ratio by Quality')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr = data.corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap of Features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K19Fi4T1_6P"
      },
      "source": [
        "### Data Cleaning\n",
        "#### Purpose\n",
        "Identify and quantify outliers using the Interquartile Range (IQR) method.\n",
        "\n",
        "#### Thought Process\n",
        "- Outliers are defined as points below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\n",
        "- We’ll print the number of outliers per column but not remove them, as tree-based models (e.g., RandomForest) are robust to outliers.\n",
        "\n",
        "#### Expected Observation\n",
        "Some features may have outliers (e.g., `APVs - Specific injection pressure peak value`). Since we’re using robust models, we’ll keep them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LT-qBeF2A1f"
      },
      "outputs": [],
      "source": [
        "## Data Cleaning\n",
        "print(\"### Data Cleaning\")\n",
        "Q1 = data.quantile(0.25)\n",
        "Q3 = data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum()\n",
        "print(\"Outliers per column:\\n\", outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERjOMTVA2OiS"
      },
      "source": [
        "### Train/Test Split and Standardization\n",
        "#### Purpose\n",
        "Split the data into training and testing sets, and standardize the features.\n",
        "\n",
        "#### Thought Process\n",
        "- Split the data into training (70%) and testing (30%) sets using `train_test_split` with `random_state` for reproducibility.\n",
        "- Standardize features using `StandardScaler` to ensure all features are on the same scale, which is important for models like KNN and ANN.\n",
        "\n",
        "#### Expected Observation\n",
        "Data should be split into 70% training and 30% testing sets. Features should be standardized (mean=0, std=1) to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tHZazka2QkU"
      },
      "outputs": [],
      "source": [
        "## Train/Test Split and Standardization\n",
        "X = data.drop('quality', axis=1)\n",
        "y = data['quality']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQ58Ozb2U8q"
      },
      "source": [
        "## Section 2: Hypothesis Testing & ANOVA (15%)\n",
        "### Purpose\n",
        "Use ANOVA to test if each feature varies significantly across the four quality levels (1-4).\n",
        "\n",
        "### Thought Process\n",
        "- **Null Hypothesis (H0)**: The feature’s mean is the same across all quality levels.\n",
        "- **Alternative Hypothesis (H1)**: The feature’s mean differs across quality levels.\n",
        "- A low p-value (< 0.05) indicates statistical significance.\n",
        "\n",
        "### Expected Observation\n",
        "Features with p-values < 0.05 (e.g., `Melt temperature`) should be statistically significant predictors of quality, confirming their importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoXjNG6A2XzV"
      },
      "outputs": [],
      "source": [
        "# Section 2: Hypothesis Testing & ANOVA (15%)\n",
        "print(\"\\n## Section 2: Hypothesis Testing & ANOVA\")\n",
        "print(\"### ANOVA Results\")\n",
        "for col in X.columns:\n",
        "    groups = [X[col][y == i] for i in range(1, 5)]  # Quality: 1-4\n",
        "    f_stat, p_val = f_oneway(*groups)\n",
        "    print(f\"{col}: F-Statistic = {f_stat:.2f}, p-value = {p_val:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKib4Co02cLa"
      },
      "source": [
        "## Section 3: Machine Learning Model Development (40%)\n",
        "### Purpose\n",
        "Train and evaluate multiple models to predict `quality`, including RandomForest, ExtraTrees, KNN, LightGBM, and ANN (mandatory).\n",
        "\n",
        "### Thought Process\n",
        "- Use `GridSearchCV` with 3-fold cross-validation to tune hyperparameters.\n",
        "- Evaluate models using accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        "### Expected Observation\n",
        "Models should train successfully. LightGBM previously showed the highest accuracy (0.926667). ANN performance will depend on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FguDAU212jYa"
      },
      "source": [
        "### Evaluation Function\n",
        "#### Purpose\n",
        "Define a function to compute multiple metrics for consistent evaluation across models.\n",
        "\n",
        "#### Thought Process\n",
        "- Compute accuracy, precision, recall, and F1-score for overall performance.\n",
        "- Compute ROC-AUC for multi-class classification performance.\n",
        "\n",
        "#### Expected Observation\n",
        "The evaluation function should be defined to compute all required metrics, ensuring fair comparison across models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wnXZreo2lki"
      },
      "outputs": [],
      "source": [
        "## Evaluation Function\n",
        "def evaluate_model(y_true, y_pred, y_prob=None):\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred, average='weighted'),\n",
        "        'Recall': recall_score(y_true, y_pred, average='weighted'),\n",
        "        'F1-Score': f1_score(y_true, y_pred, average='weighted')\n",
        "    }\n",
        "    if y_prob is not None:\n",
        "        metrics['ROC-AUC'] = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GUWBbBy2q_U"
      },
      "source": [
        "### ANN Wrapper for scikit-learn Compatibility\n",
        "#### Purpose\n",
        "Create a wrapper class for the ANN to make it compatible with `GridSearchCV` for consistent model comparison.\n",
        "\n",
        "#### Thought Process\n",
        "- The ANN has two hidden layers (64 and 32 neurons) with ReLU activation, and an output layer with softmax for 4 classes.\n",
        "- Compile with Adam optimizer and sparse categorical crossentropy loss.\n",
        "\n",
        "#### Expected Observation\n",
        "The `ANNClassifier` wrapper should allow the ANN to be used with `GridSearchCV`, ensuring it’s evaluated like other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u66ftrQj2sXJ"
      },
      "outputs": [],
      "source": [
        "## ANN Wrapper for scikit-learn Compatibility\n",
        "class ANNClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, epochs=20, batch_size=32):\n",
        "        self.input_dim = input_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(self.input_dim,)),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(4, activation='softmax')\n",
        "        ])\n",
        "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_prob = self.model.predict(X, verbose=0)\n",
        "        return np.argmax(y_prob, axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict(X, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMca6Ccy2zTB"
      },
      "source": [
        "### Model Training and Evaluation\n",
        "#### Purpose\n",
        "Define models and hyperparameters, then train and evaluate them using `GridSearchCV`.\n",
        "\n",
        "#### Thought Process\n",
        "- Models: RandomForest, ExtraTrees, KNN, LightGBM, and ANN.\n",
        "- Use 3-fold cross-validation to tune hyperparameters.\n",
        "- Adjust `y_train` and `y_pred` for 0-based indexing (TensorFlow expects 0-3, but dataset has 1-4).\n",
        "\n",
        "#### Expected Observation\n",
        "LightGBM previously showed the highest accuracy (0.926667). ANN performance will depend on the dataset, but expect it to be competitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q-_1VI820AN"
      },
      "outputs": [],
      "source": [
        "## Model Training and Evaluation\n",
        "models = {\n",
        "    'RandomForest': (RandomForestClassifier(), {'n_estimators': [50, 100], 'max_depth': [10, 20]}),\n",
        "    'ExtraTrees': (ExtraTreesClassifier(), {'n_estimators': [50, 100], 'max_depth': [10, 20]}),\n",
        "    'KNN': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
        "    'LightGBM': (LGBMClassifier(), {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}),\n",
        "    'ANN': (ANNClassifier(input_dim=X_train_scaled.shape[1]), {'epochs': [20], 'batch_size': [32]})\n",
        "}\n",
        "\n",
        "results = {}\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for name, (model, params) in models.items():\n",
        "    print(f\"\\n### Training {name}\")\n",
        "    grid = GridSearchCV(model, params, cv=kf, scoring='accuracy')\n",
        "    grid.fit(X_train_scaled, y_train - 1)\n",
        "    y_pred = grid.predict(X_test_scaled) + 1\n",
        "    y_prob = grid.predict_proba(X_test_scaled)\n",
        "    results[name] = evaluate_model(y_test, y_pred, y_prob)\n",
        "    print(f\"Best Params: {grid.best_params_}\")\n",
        "    print(f\"Results: {results[name]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0VaC3wX3E8F"
      },
      "source": [
        "### Feature Importance (Random Forest)\n",
        "#### Purpose\n",
        "Use RandomForest to compute feature importance, identifying which features contribute most to quality prediction.\n",
        "\n",
        "#### Thought Process\n",
        "- Train a RandomForest model with the best hyperparameters found (`n_estimators=100`, `max_depth=20`).\n",
        "- Plot feature importance as a horizontal bar chart.\n",
        "\n",
        "#### Expected Observation\n",
        "Features like `Melt temperature` and `pressure_ratio` may rank high, aligning with ANOVA results, indicating their predictive power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXxWZzq73FsG"
      },
      "outputs": [],
      "source": [
        "## Feature Importance (Random Forest)\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=20).fit(X_train_scaled, y_train - 1)\n",
        "feat_importance = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "plt.figure(figsize=(10, 6))\n",
        "feat_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAGWDmGM3M91"
      },
      "source": [
        "### Model Comparison\n",
        "#### Purpose\n",
        "Compile the evaluation results into a table for easy comparison across models.\n",
        "\n",
        "#### Thought Process\n",
        "- Include all models: RandomForest, ExtraTrees, KNN, LightGBM, and ANN.\n",
        "- Metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC.\n",
        "\n",
        "#### Expected Observation\n",
        "LightGBM previously outperformed others (Accuracy: 0.926667). ANN should be competitive, but its performance depends on the dataset complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BolDD5kY3PFd"
      },
      "outputs": [],
      "source": [
        "## Model Comparison\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\n### Model Comparison\\n\", results_df)\n",
        "\n",
        "# Bar Chart for Model Accuracy\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Accuracy', y=results_df.index, data=results_df, palette='viridis')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Model')\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrices for Each Model\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 15))\n",
        "fig.suptitle('Confusion Matrices for Each Model', fontsize=16)\n",
        "\n",
        "# Define quality labels for the confusion matrix\n",
        "quality_labels = ['Waste', 'Acceptable', 'Target', 'Inefficient']\n",
        "\n",
        "for idx, (model_name, y_pred) in enumerate(model_predictions.items()):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[row, col],\n",
        "                xticklabels=quality_labels, yticklabels=quality_labels)\n",
        "    axes[row, col].set_title(f'Confusion Matrix: {model_name}')\n",
        "    axes[row, col].set_xlabel('Predicted')\n",
        "    axes[row, col].set_ylabel('Actual')\n",
        "\n",
        "# Remove the empty subplot (if any)\n",
        "if len(model_predictions) % 2 != 0:\n",
        "    fig.delaxes(axes[-1, -1])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brsxa0BR3YZS"
      },
      "source": [
        "## Section 4: Interactive Dashboard Development (20%)\n",
        "### Purpose\n",
        "Create a Streamlit dashboard for interactive quality prediction.\n",
        "\n",
        "### Thought Process\n",
        "- Allow users to input key features via sliders.\n",
        "- Use the trained RandomForest model for prediction.\n",
        "- Display prediction probabilities, feature importance, and confusion matrix.\n",
        "\n",
        "### Expected Observation\n",
        "The dashboard should allow users to input key features and see the predicted quality, along with visualizations for interpretability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEkqAjU43bXP"
      },
      "outputs": [],
      "source": [
        "# Section 4: Interactive Dashboard Development (20%)\n",
        "print(\"\\n## Section 4: Interactive Dashboard Development\")\n",
        "dashboard_code = \"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load and preprocess data with encoding handling\n",
        "try:\n",
        "    data = pd.read_csv('dataset.csv', encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    data = pd.read_csv('dataset.csv', encoding='latin1')\n",
        "\n",
        "data['pressure_ratio'] = data['APVs - Specific injection pressure peak value'] / data['APSs - Specific back pressure peak value']\n",
        "X = data.drop('quality', axis=1)\n",
        "y = data['quality']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train RandomForest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=20)\n",
        "rf_model.fit(X_scaled, y - 1)\n",
        "\n",
        "# Dashboard\n",
        "st.title('Plastic Injection Moulding Quality Predictor')\n",
        "\n",
        "# User Inputs\n",
        "melt_temp = st.slider('Melt Temperature (°C)', float(X['Melt temperature'].min()), float(X['Melt temperature'].max()), 106.0)\n",
        "mold_temp = st.slider('Mold Temperature (°C)', float(X['Mold temperature'].min()), float(X['Mold temperature'].max()), 81.0)\n",
        "time_to_fill = st.slider('Time to Fill (s)', float(X['time_to_fill'].min()), float(X['time_to_fill'].max()), 6.5)\n",
        "shot_volume = st.slider('Shot Volume (cm³)', float(X['SVo - Shot volume'].min()), float(X['SVo - Shot volume'].max()), 18.7)\n",
        "\n",
        "# Prepare input (values are approximate medians from dataset)\n",
        "input_data = pd.DataFrame([[\n",
        "    melt_temp,  # Melt temperature\n",
        "    mold_temp,  # Mold temperature\n",
        "    time_to_fill,  # time_to_fill\n",
        "    shot_volume,  # SVo - Shot volume\n",
        "    550,  # APVs - Specific injection pressure peak value\n",
        "    275,  # APSs - Specific back pressure peak value\n",
        "    5.5,  # ZUX - Cycle time\n",
        "    3.0,  # ZDx - Plasticizing time\n",
        "    100,  # SKs - Clamping force peak value\n",
        "    100,  # SKx - Closing force peak value\n",
        "    5.5,  # CPn - Screw position at the end of hold pressure\n",
        "    10,   # Mm - Torque mean value current cycle\n",
        "    10,   # Ms - Torque peak value current cycle\n",
        "    550/275  # pressure_ratio (APVs / APSs)\n",
        "]], columns=X.columns)\n",
        "input_scaled = scaler.transform(input_data)\n",
        "\n",
        "# Prediction\n",
        "pred = rf_model.predict(input_scaled)[0] + 1\n",
        "probs = rf_model.predict_proba(input_scaled)[0]\n",
        "quality_map = {1: 'Waste', 2: 'Acceptable', 3: 'Target', 4: 'Inefficient'}\n",
        "st.write(f'Predicted Quality: **{quality_map[pred]}**')\n",
        "\n",
        "# Layout for plots\n",
        "col1, col2, col3 = st.columns(3)\n",
        "\n",
        "# Probability Bar Chart\n",
        "with col1:\n",
        "    st.subheader('Prediction Probabilities')\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    ax.bar(quality_map.values(), probs)\n",
        "    ax.set_ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# Feature Importance\n",
        "with col2:\n",
        "    st.subheader('Feature Importance')\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    feat_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
        "    feat_importance.sort_values().plot(kind='barh', ax=ax)\n",
        "    ax.set_title('Feature Importance')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# Confusion Matrix\n",
        "with col3:\n",
        "    st.subheader('Confusion Matrix')\n",
        "    y_pred_full = rf_model.predict(X_scaled) + 1\n",
        "    cm = confusion_matrix(y, y_pred_full)\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Actual')\n",
        "    st.pyplot(fig)\n",
        "\"\"\"\n",
        "\n",
        "# Write the dashboard file\n",
        "with open('dashboard.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(dashboard_code)\n",
        "print(\"Dashboard code saved as 'dashboard.py'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfbDFC5E3kdM"
      },
      "source": [
        "### Launch Streamlit Dashboard\n",
        "#### Purpose\n",
        "Launch the Streamlit dashboard from Jupyter Notebook.\n",
        "\n",
        "#### Thought Process\n",
        "- Use `subprocess` to run the Streamlit app.\n",
        "- The dashboard should open at `http://localhost:8501`.\n",
        "\n",
        "#### Expected Observation\n",
        "The dashboard should launch at `http://localhost:8501`, providing an interactive interface for quality prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8fvagQe3mD3"
      },
      "outputs": [],
      "source": [
        "## Launch Streamlit Dashboard\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "print(\"Launching Streamlit dashboard...\")\n",
        "try:\n",
        "    process = subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", \"dashboard.py\"],\n",
        "                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    time.sleep(3)\n",
        "    print(\"Dashboard should be running. Open your browser to: http://localhost:8501\")\n",
        "    print(\"To stop the dashboard, interrupt the kernel (e.g., press 'I, I' in Jupyter or Ctrl+C in terminal).\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to launch dashboard: {e}\")\n",
        "    print(\"Try running 'streamlit run dashboard.py' manually in a terminal in the same directory.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}